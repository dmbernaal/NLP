{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling & Text Generation from IMDB\n",
    "We will be looking at IMDB movie reviews. We want to determine if a review is negative or positive, based on the text. In order to do this, we will be using **Transfer Learning**\n",
    "\n",
    "Transfer learning has been widely used with great success in computer vision for several years, but only in the last yar or so has it been successfully applied to NLP.\n",
    "\n",
    "# Using a GPU\n",
    "In this notebook we will be using a RTX 2070-MaxQ GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2070 with Max-Q Design'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing our device name - you will need a GPU\n",
    "print(torch.cuda.device_count()) # how many GPUs?\n",
    "\n",
    "device_ = torch.cuda.current_device() # get current device\n",
    "torch.cuda.get_device_name(device_) # printing the device name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the device\n",
    "torch.cuda.set_device(device_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting our batch-size, depending on the memory we may need to decrease this\n",
    "bs = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data (on a sample)\n",
    "We will first download the dataset we will use which contains a total of 100,000 reviews on IMDB. 25,000 of them are labelled as positive and negative for training, another 25,000 are labelled for testing. The remaining 50,000 is an additional unlabelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/dmber/.fastai/data/imdb_sample/texts.csv')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the path of our data\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, this only contains one csv file. It contains one line per review, with the label (negative, positive), the text and a flag to determine if it should be part of the validation set or the training set. If we ignore this flag, we can create a DataBunch containing this data in one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DataBunch API\n",
    "data_lm = TextDataBunch.from_csv(path, 'texts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images could be fed (almost) directly into a model because they're just a big array of pixel values that are floats between 0 and 1. A text is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two different steps: **Tokenization** and **Numericalization**. ```TextDataBunch``` does this all for us behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "The first step of processing we make texts go through is to split the raw sentences into words, or more exactly **tokens**. The easiest way to do this would be to split the string into space, but ```TextDataBunch``` does more:\n",
    "* We need to take care of punctuation\n",
    "* Some words are contractions of two different words, like isn't or don't\n",
    "* We may need to clean some parts of our texts, if there's HTML code for instance\n",
    "\n",
    "The texts are truncated at 100 tokens for more readability. We can see that it did more than jsut split on space and punctuation symbols:\n",
    "* The \"s\" are grouped together in one token\n",
    "* The contraction are seperated like this: \"did\", \"n't\"\n",
    "* The content has been cleaned for any HMTL symbol and lower cased\n",
    "* There are several special tokens\n",
    "\n",
    "## Numericalization\n",
    "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token ```UNK```.\n",
    "\n",
    "The correspondance from ids tokens is stored in the ```vocab``` attribute of our datasets, in a dictionary called ```itos``` (int to string). This is actually a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxfld',\n",
       " 'xxmaj',\n",
       " 'xxup',\n",
       " 'xxrep',\n",
       " 'xxwrep',\n",
       " 'the',\n",
       " ',',\n",
       " '.',\n",
       " 'and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'it',\n",
       " 'in',\n",
       " 'i']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grabbing the first 20 tokens in our vocab\n",
    "data_lm.vocab.itos[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8880"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the length of our vocab\n",
    "len(data_lm.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text xxbos xxmaj this was the first xxmaj ewan mcgregor movie i ever saw outside of xxmaj star xxmaj wars . xxmaj since then i have become a very big xxmaj ewan mcgregor fan but i still ca n't bring myself to forgive this movie 's existence . \n",
       " \n",
       "  xxmaj my sister has always been a huge xxmaj jane xxmaj austen fan and because of that , i have been subjected to various of the classics , xxmaj emma being one of them . i 've always considered them irritating , stupid and boring . xxmaj however , after watching this terrible rendition , i was forced to admit that the original xxmaj emma was delightful and charming . xxmaj ewan mcgregor scarcely serves a purpose in this film after they hacked and xxunk the part of xxmaj frank xxmaj xxunk . xxmaj xxunk xxmaj paltrow is ridiculous in an already ridiculous character and the rest of the film is xxunk and stupid . \n",
       " \n",
       "  xxmaj my recommendation to anybody who is remotely interested in xxmaj english period drama ... go see the originals . xxmaj if you 're a xxmaj ewan mcgregor fan ... believe me , by skipping this film , you have n't missed anything but five minutes collective of him in a silly hat and a bad xxunk ."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check one example from our train dataset\n",
    "data_lm.train_ds[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative, Flexible Approach\n",
    "We can use the data block API with NLP and have a lot more flexibility than what the default factory methods offer. In the previous examples for instance, the data was randomly split between train and validation instead of reading the third column of the csv.\n",
    "\n",
    "With the datablock API though, we have to manually call the tokenize and numerical steps. This allows more flexibility, and if you're not using the defaults from fastai, the various arguments to pass will appear in the step they're revelant, so it'll be more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
    "                .split_from_df(col=2)\n",
    "                .label_from_df(cols=0)\n",
    "                .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxfld',\n",
       " 'xxmaj',\n",
       " 'xxup',\n",
       " 'xxrep',\n",
       " 'xxwrep',\n",
       " 'the']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6016"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see there are less vocab because we didn't feed any manual parameters into ```.databunch``` which you can. Just look at the FASTAI documentation to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Dataset - Language Model\n",
    "Now we will do the same but with the larger full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while ~ 5 min\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/dmber/.fastai/data/imdb/imdb.vocab'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/README'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/test'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/tmp_clas'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/tmp_lm'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/train'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/unsup')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/dmber/.fastai/data/imdb/train/labeledBow.feat'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/train/neg'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/train/pos'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/train/unsupBow.feat')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking our train directory\n",
    "(path/\"train\").ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews are in a training and test set following an imagenet structure. The only difference is that there is an ```unsup``` folder that contains unlabelled data.\n",
    "\n",
    "We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model **pretrained** on a bigger dataset (wikitext-103). That model has been trained to guess what the next word, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point. \n",
    "\n",
    "We're going to use that \"knowledge\" of the english language to build or classifier, but first, like for computer vision, we need to fine-tune the pre-trained model to our particular dataset. Because the english of the reviews left by people on IMDB isn't the same as the english of wikipedia, we'll need to adjust a little bit of the parameters of our model. Plus there might be some words extremely common in that dataset that were barely present in wikipedia, and therefor might not be part of the vocabulary the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the ```TextLMDataBunch```\n",
    "This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the datablock API.\n",
    "\n",
    "As you will notice, we are not trying to predict **sentiment** in the following cells, but are actually predicting the **next word** to build a fine-tuned language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_folder(path)\n",
    "                    # Using all text files in our path : train and test\n",
    "                   .filter_by_folder(include=['train', 'test'])\n",
    "                    # We will keep 10% for validation\n",
    "                   .random_split_by_pct(0.1)\n",
    "                    # Telling DB API we are labeling for a language model\n",
    "                   .label_for_lm()\n",
    "                    # specifying our batchsize\n",
    "                   .databunch(bs=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49848"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the size of our new vocab\n",
    "len(data_lm.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>later , by which time i did not care . xxmaj the character we should really care about is a very cocky , overconfident xxmaj ashton xxmaj kutcher . xxmaj the problem is he comes off as kid who thinks he 's better than anyone else around him and shows no signs of a cluttered closet . xxmaj his only obstacle appears to be winning over xxmaj costner . xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>, idea for the movie . \\n \\n  xxmaj overall , this flick deserves 4 / 10 from me . xxmaj it 's not as bad as people say . xxmaj imagine a xxup zombie xxup western , then watch this movie . xxbos xxmaj produced by xxmaj nott xxmaj entertainment , this movie is \" nott \" very good at all . i sat through the first 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>xxmaj the rest of the aggressive bunch are xxmaj askew ( one of the only three survivors ) , xxmaj xxunk , xxmaj lucking , xxmaj lauter ; xxmaj rita xxmaj rogers is truly hot , fleshy beauty . xxbos xxmaj the director was probably still in his early learning stages when he tried his hand at westerns . xxmaj have a look at the outfits . xxmaj everybody looks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just like a df we can show part of our file\n",
    "data_lm.show_batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save our databunch so we can pickup where we left off\n",
    "data_lm.save('lm_databunch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How to load the databunch\n",
    "data_lm = load_data(path, 'lm_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj once again xxmaj mr. xxmaj costner has dragged out a movie for far longer than necessary . xxmaj aside from the terrific sea rescue sequences , of which there are very few i just did not care about any of the characters . xxmaj most of us have ghosts in the closet , and xxmaj costner 's character are realized early on , and then forgotten until much later , by which time i did not care . xxmaj the character we should really care about is a very cocky , overconfident xxmaj ashton xxmaj kutcher . xxmaj the problem is he comes off as kid who thinks he 's better than anyone else around him and shows no signs of a cluttered closet . xxmaj his only obstacle appears to be winning over xxmaj costner . xxmaj finally when we are well past the half way point of this stinker , xxmaj costner tells us all about xxmaj kutcher 's ghosts . xxmaj we are told why xxmaj kutcher is driven to be the best with no prior inkling or foreshadowing . xxmaj no magic here , it was all i could do to keep from turning it off an hour in .,\n",
       " EmptyLabel )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model - pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our language model - Pretrained\n",
    "learn_lm = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       " \u001b[0mAWD_LSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvocab_sz\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0memb_sz\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_hid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_layers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpad_token\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mhidden_p\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minput_p\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0membed_p\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mweight_p\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mqrnn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbidir\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      AWD-LSTM/QRNN inspired by https://arxiv.org/abs/1708.02182.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\dmber\\anaconda3\\lib\\site-packages\\fastai\\text\\models\\awd_lstm.py\n",
       "\u001b[1;31mType:\u001b[0m           PrePostInitMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# want to know more about this pre-trained model?\n",
    "# https://arxiv.org/pdf/1708.02182.pdf\n",
    "? AWD_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on ```AWD_LSTM``` \n",
    "AWD_LSTM(vocab_sz:int, emb_sz:int, n_hid:int, n_layers:int, pad_token:int=1,hidden_p:float=0.2, input_p:float=0.6,embed_p:float=0.1,weight_p:float=0.5, qrnn:bool=False, bidir:bool=False)\n",
    "\n",
    "*directly from FASTAI documentation*\n",
    "The main idea from the white-paper implementation of this model is to use an **RNN** with dropout everywhere, but in an intelligent way. \n",
    "\n",
    "There is a difference with the usual dropout, which is why we will see ```RNNDropout``` module: we zero things, as in usual dropout, but we always zero the same thing accordingly to the sequence dimension (when is the first dimension in PyTorch). This ensures consistency when updating the hidden state through the whole sentence/articles.\n",
    "\n",
    "There are a total of four different dropouts in the encoder of the AWD-LSTM:\n",
    "* the first one, embedding dropout, is applied when we look the ids of our tokens inside the embedding matrix (to transform them from numbers to a vector of float). We zero some lines of it, so random ids are sent to a vector of zeros instead of being sent to their embedding vector. This is the ```embed_p``` parameter\n",
    "* the second one, input dropout, is applied to the result of the embedding with dropout. We forget random pieces of the embedding matrix (but as stated in the last paragraph, the same ones in the sequence dimension). This is the ```input_p``` parameter.\n",
    "* the third one, is the weight dropout. It's the trickiest to implement as we randomly replace by 0s some weights of the hidden-to-hidden matrix inside the RNN: this needs to be done is a way that ensures the gradients are still computed and the initial weights still updated. This is the ```weight_p``` parameter.\n",
    "* the fourth one, is the hidden dropout. It's applied to the output of one of the layers of the RNN before it's used as input of the next layer (again same coordinates are zeroed in the sequence dimension). It isn't applied to the last output (which will get its own dropout in the decoder). This is the ```hidden_p``` parameter.\n",
    "\n",
    "The other attributes are:\n",
    "* ```vocab_sz```: number of tokens in your vocabulary\n",
    "* ```emb_sz```: the embedding size\n",
    "* ```n_hid```: for the hidden size of your inner LSTMs (or QRNNs)\n",
    "* ```n_layers```: the number of layers - how many LSTM cells\n",
    "* ```pad_token```: for the index of an eventual padding token (1 by default)\n",
    "\n",
    "If you use: ```qrnn=True``` you will replace inner LSTMs by QRNNs. \n",
    "\n",
    "**QRNNs** are: Quasi-Recurrent Neural Networks. To learn more visit: https://arxiv.org/abs/1611.01576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can actually override the AWS_LSTM model by calling a new model and calling super on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(49848, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(49848, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=49848, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_lm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        module\n",
       "\u001b[1;31mString form:\u001b[0m <module 'fastai.text.models' from 'C:\\\\Users\\\\dmber\\\\Anaconda3\\\\lib\\\\site-packages\\\\fastai\\\\text\\\\models\\\\__init__.py'>\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\dmber\\anaconda3\\lib\\site-packages\\fastai\\text\\models\\__init__.py\n",
       "\u001b[1;31mSource:\u001b[0m     \n",
       "\u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mawd_lstm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mawd_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To view all the models just run: \n",
    "# text.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Wikitext-103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling our wiki vocab\n",
    "wiki_itos = pickle.load(open(Config().model_path()/'wt103-fwd/itos_wt103.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxfld',\n",
       " 'xxmaj',\n",
       " 'xxup',\n",
       " 'xxrep',\n",
       " 'xxwrep',\n",
       " 'the']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the wiki text vocab\n",
    "len(wiki_itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing our vocab\n",
    "vocab = data_lm.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49848"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Fake Movie Reviews (Using Wiki-Text Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       " \u001b[0mlearn_lm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_words\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mno_unk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_p\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdecoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mdecode_spec_tokens\u001b[0m \u001b[0mat\u001b[0m \u001b[1;36m0x000001DC2581D7B8\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m Return the `n_words` that come after `text`.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\dmber\\anaconda3\\lib\\site-packages\\fastai\\text\\learner.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We call learn_lm.predict() - to generate our text\n",
    "? learn_lm.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_review(language_model_, text_, n_words_=40, n_sentences_=2, temp_=0.75):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "        language_model_: <language model object>\n",
    "        text_: <str>: Beginning text, generation will occur from this\n",
    "        n_words_: <int>: Number of words to generate\n",
    "        n_sententences_: <int>: Number of sentences to generate\n",
    "        temp_: <float>: How much randomness?\n",
    "        \n",
    "    RETURN:\n",
    "        Will return generated text from language model\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(language_model_.predict(text_, n_words_, temperature=temp_) for _ in range(n_sentences_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoking a juul before going to bed is good because of the lack of appetite for the gatherings . On playhouse , a young man who raced was called the Son of Halloween . interesting people , such as over - age men who want to eat eating merchandise interspersed with spending locations , occur frequently ,\n"
     ]
    }
   ],
   "source": [
    "text = \"Smoking a juul before going to bed is good because\"\n",
    "generated_text = generate_fake_review(learn_lm, text, n_words_=50, n_sentences_=1, temp_=0.99)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finding the optimal learning rate\n",
    "learn_lm.lr_find()\n",
    "learn_lm.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 08:25 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.152680</td>\n",
       "      <td>4.953022</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>08:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "learn_lm.fit_one_cycle(1, lr, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "learn_lm.save('fit_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (45000 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj once again xxmaj mr. xxmaj costner has dragged out a movie for far longer than necessary . xxmaj aside from the terrific sea rescue sequences , of which there are very few i just did not care about any of the characters . xxmaj most of us have ghosts in the closet , and xxmaj costner 's character are realized early on , and then forgotten until much later , by which time i did not care . xxmaj the character we should really care about is a very cocky , overconfident xxmaj ashton xxmaj kutcher . xxmaj the problem is he comes off as kid who thinks he 's better than anyone else around him and shows no signs of a cluttered closet . xxmaj his only obstacle appears to be winning over xxmaj costner . xxmaj finally when we are well past the half way point of this stinker , xxmaj costner tells us all about xxmaj kutcher 's ghosts . xxmaj we are told why xxmaj kutcher is driven to be the best with no prior inkling or foreshadowing . xxmaj no magic here , it was all i could do to keep from turning it off an hour in .,xxbos xxmaj this is an example of why the majority of action films are the same . xxmaj generic and boring , there 's really nothing worth watching here . a complete waste of the then barely - tapped talents of xxmaj ice - t and xxmaj ice xxmaj cube , who 've each proven many times over that they are capable of acting , and acting well . xxmaj do n't bother with this one , go see xxmaj new xxmaj jack xxmaj city , xxmaj ricochet or watch xxmaj new xxmaj york xxmaj undercover for xxmaj ice - t , or xxmaj boyz n the xxmaj hood , xxmaj higher xxmaj learning or xxmaj friday for xxmaj ice xxmaj cube and see the real deal . xxmaj ice - t 's horribly cliched dialogue alone makes this film grate at the teeth , and i 'm still wondering what the heck xxmaj bill xxmaj paxton was doing in this film ? xxmaj and why the heck does he always play the exact same character ? xxmaj from xxmaj aliens onward , every film i 've seen with xxmaj bill xxmaj paxton has him playing the exact same irritating character , and at least in xxmaj aliens his character died , which made it somewhat gratifying ... \n",
       " \n",
       "  xxmaj overall , this is second - rate action trash . xxmaj there are countless better films to see , and if you really want to see this one , watch xxmaj judgement xxmaj night , which is practically a carbon copy but has better acting and a better script . xxmaj the only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing , which comes close to making up for the horrible film itself - but not quite . 4 / 10 .,xxbos xxmaj first of all i hate those moronic rappers , who could'nt act if they had a gun pressed against their foreheads . xxmaj all they do is curse and shoot each other and acting like xxunk version of gangsters . \n",
       " \n",
       "  xxmaj the movie does n't take more than five minutes to explain what is going on before we 're already at the warehouse xxmaj there is not a single sympathetic character in this movie , except for the homeless guy , who is also the only one with half a brain . \n",
       " \n",
       "  xxmaj bill xxmaj paxton and xxmaj william xxmaj sadler are both hill billies and xxmaj xxunk character is just as much a villain as the gangsters . i did'nt like him right from the start . \n",
       " \n",
       "  xxmaj the movie is filled with pointless violence and xxmaj walter xxmaj hills specialty : people falling through windows with glass flying everywhere . xxmaj there is pretty much no plot and it is a big problem when you root for no - one . xxmaj everybody dies , except from xxmaj paxton and the homeless guy and everybody get what they deserve . \n",
       " \n",
       "  xxmaj the only two black people that can act is the homeless guy and the junkie but they 're actors by profession , not annoying ugly brain dead rappers . \n",
       " \n",
       "  xxmaj stay away from this crap and watch 48 hours 1 and 2 instead . xxmaj at lest they have characters you care about , a sense of humor and nothing but real actors in the cast .,xxbos xxmaj not even the xxmaj beatles could write songs everyone liked , and although xxmaj walter xxmaj hill is no mop - top he 's second to none when it comes to thought provoking action movies . xxmaj the nineties came and social xxunk were changing in music and film , the emergence of the xxmaj rapper turned movie star was in full swing , the acting took a back seat to each man 's overpowering regional accent and transparent acting . xxmaj this was one of the many ice - t movies i saw as a kid and loved , only to watch them later and cringe . xxmaj bill xxmaj paxton and xxmaj william xxmaj sadler are firemen with basic lives until a burning building tenant about to go up in flames hands over a map with gold implications . i hand it to xxmaj walter for quickly and neatly setting up the main characters and location . xxmaj but i fault everyone involved for turning out xxmaj lame - o performances . xxmaj ice - t and cube must have been red hot at this time , and while i 've enjoyed both their careers as rappers , in my opinion they fell flat in this movie . xxmaj it 's about ninety minutes of one guy ridiculously turning his back on the other guy to the point you find yourself locked in multiple states of disbelief . xxmaj now this is a movie , its not a documentary so i wo nt waste my time recounting all the stupid plot twists in this movie , but there were many , and they led nowhere . i got the feeling watching this that everyone on set was xxunk of confused and just playing things off the cuff . xxmaj there are two things i still enjoy about it , one involves a scene with a needle and the other is xxmaj sadler 's huge 45 pistol . xxmaj bottom line this movie is like domino 's pizza . xxmaj yeah ill eat it if i 'm hungry and i do n't feel like cooking , xxmaj but i 'm well aware it tastes like crap . 3 stars , meh .,xxbos a funny thing happened to me while watching \" xxmaj mosquito \" : on the one hand , the hero is a deaf - mute and the director is totally unable to make us understand why he does what he does ( mutilating mannequins ... er , excuse me , corpses ) through his images . xxmaj on the other hand , the xxmaj english version at least is very badly dubbed . xxmaj so i found myself wishing there had been both more xxup and less dialogue at the same time ! xxmaj this film is stupid ( funny how this guy has access to every graveyard and mortuary in his town ) and lurid ( where would we be in a 70s xxunk without our gratuitous lesbian scene ? ) . xxmaj not to mention the \" romantic \" aspect ( oh , how xxunk ... xxmaj miss it . ( * )\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: C:\\Users\\dmber\\.fastai\\data\\imdb;\n",
       "\n",
       "Valid: LabelList (5000 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj cute idea ... salesgirl xxmaj linda xxmaj smith ( xxmaj yolande xxmaj donlan ) inherits a teeny tiny little county of xxmaj xxunk . xxmaj that country , which was n't even in xxmaj north xxmaj america , was made the 49th state ... ( of course , there were only 48 states at the time , since this was made in 1952 ... ) xxmaj linda travels to the country she has inherited , and we follow her along as she tries to figure out what to do with this strange country and its even quirkier people . xxmaj at one point , she sings a song that she claims is from her people the xxmaj navajo , and it gets ever - more sillier from there xxrep 4 . although xxmaj yolande xxmaj donlan 's heavy lipstick and omni - present smile never get ruffled or xxunk . xxmaj there are other songs scattered through - out as the citizens sing to welcome their new princess . xxmaj filmed in a glorious xxmaj british version of technicolor , or some such equivalent , about the only big name here is xxmaj dirk xxmaj bogarde as xxmaj british subject xxmaj tony xxmaj craig , cheese vendor . xxmaj bogarde made a big splash in the xxup uk film industry after serving in the war , and was even knighted by xxup xxunk xxup ii . xxmaj craig and \" the new princess \" keep bumping into each other , and their adventures become more intertwined as xxmaj xxunk 's financial problems worsen ... xxmaj fun little farce xxrep 4 . along the lines of xxmaj marx xxmaj brothers film . xxmaj also note that xxmaj donlan later married xxmaj val xxmaj guest , the writer and director of our little project , and stayed married for 50 years ! xxmaj guest was better known for writing and directing his sci - fi flicks , in both the xxup uk & the xxup us .,xxbos xxmaj simply put : the movie is boring . xxmaj clich upon clich is confirmed and story lines never come together . xxmaj it seems as if the director was unsure whether to make a movie or a documentary . xxmaj the main plot is very thin ( a xxup cia agent is ordered to kill an oil prince , gets caught and then warns the prince ( why ? ) ) and therefore some elements were added to make the movie more interesting . xxmaj so , a kid dies , which results in the \" natural \" response of the father : freely advising the person indirectly responsible for his son 's death . xxmaj the lawyer has a drunk \" friend \" and keeps him around , why , no one knows . xxmaj some kids become suicide terrorists and blow up a ship . \n",
       " \n",
       "  xxmaj all in all , this is one of the worst movies i have seen in quite a while . i was neither entertained nor intellectually challenged . i neither laughed nor cried , i did not gain an understanding nor was i compelled to learn more or take up a cause . xxmaj it meant nothing to me , which in my eyes is the worst one can say about a movie .,xxbos xxmaj leland xxmaj fitzgerald ( xxmaj ryan xxmaj gosling ) is sent to jail for the murder of an autistic kid . xxmaj when pressured with the question ' xxmaj why ? ' he does n't have an answer . xxmaj while in jail he meets xxmaj pearl ( xxmaj don xxmaj cheadle ) , his teacher , who decides to take matters into his own hands and helps xxmaj leland figure out why he did it . xxmaj throughout this film we learn all about xxmaj leland 's troubled life , including his ex - girlfriend xxunk xxmaj malone ) , his famous father xxunk xxmaj xxunk his whole sad life . \n",
       " \n",
       "  xxmaj this film is xxmaj matthew xxmaj ryan xxmaj hoge 's second movie , and it is spectacular in nearly everyway . xxmaj this is one movie which will leave you thinking in the end , and wondering about how it all works . xxmaj the movie is quite dark , but if you can handle that then you will realize just how good a film it is . \n",
       " \n",
       "  xxmaj in this movie , there is no bad guy . xxmaj there is no one you can blame for anything that happens . xxmaj there 's no stereotyping , and the audience does not try to prove xxmaj leland guilty . xxmaj instead , we sit back , relax , and watch this boy 's life unfold throughout the xxunk of the movie . xxmaj all the problems depicted in the story are very real . xxmaj drug addiction , parental expectations , overwhelming sadness ; they all exist in our world . \n",
       " \n",
       "  xxmaj ryan xxmaj gosling gives one of the greatest performances of his career in this movie , as the depressed teenager xxmaj leland . xxmaj his father lives in xxmaj europe and does n't really care much about his son . xxmaj the only person he loves is xxmaj becky , but she has problems of her own . xxmaj he knows exactly what he did , but as he says in the film , ' xxmaj you want a why , but maybe there is n't one . xxmaj maybe this is something that just happened . ' xxmaj there is a why , but we do n't find out about it until the end . xxmaj as you watch the movie , the audience finds themselves amazed that such a young person could know so much about the world . xxmaj leland notices things that people tend to ignore . \n",
       " \n",
       "  a particular thought - provoking scene which really affected me was during one of xxmaj leland 's conversations with xxmaj pearl . xxmaj pearl just cheated on his wife and when xxmaj leland asks why , xxmaj pearl replies that he 's only human . xxmaj then xxmaj leland says something which never really occurs to anyone : \" xxmaj why do people only say that when they 've done something wrong ? \" \n",
       " \n",
       "  xxmaj another fantastic acting job was provided by xxmaj chris xxmaj klein . xxmaj in the film he plays xxmaj allen xxmaj harris , the boyfriend of xxmaj becky 's sister xxmaj julie ( xxmaj michelle xxmaj williams ) . xxmaj although he is not one of the main characters , i found myself amazed at how deep his character was . xxmaj you can relate to xxmaj allen a lot . xxmaj you know how much he cares for the xxmaj pollard family . xxmaj it 's as if they were his own flesh and blood . xxmaj by the end of the movie , you realize just how far he would go to help them . \n",
       " \n",
       "  xxmaj overall , this movie is a masterpiece which has been overlooked by quite a few people . xxmaj if , however , you take the time to watch it , you will most likely see that everything i 've mentioned above is true . xxmaj and once you 're finished watching it , you 'll never look at the world the same way again . \n",
       " \n",
       "  9.5 / 10,xxbos xxmaj fans of xxunk xxmaj herschell xxmaj gordon xxmaj lewis should look elsewhere if they are picking up this film for his usual buckets of blood being xxunk about , for there is precious little in the way of bloodletting in this film . xxmaj instead , xxmaj lewis decides to try and tell the bizarre story , relying on bargain - basement special effects on a budget which could have probably been doubled if the cast had turned out their pockets for change one day . xxmaj oddly enough , while cheap and very poorly acted ( especially by mccabe as xxmaj mitchell ) , the total xxunk of the plot keeps attention throughout . xxmaj imagine what this film could have been like with a decent budget ! xxmaj overall , it strains for champagne tastes on a beer budget .,xxbos i picked this xxup dvd up for 3.99 at rogers video in order to get enough points to get a better movie for free . i never actually was planning on watching this but it started poking at my curiosity and i finally decided to pop in it the xxup dvd player . xxmaj the effects in this movie are horrible and cheap . xxmaj some of the dialog in this movie sounds like it was written by a swear happy 12 year old boy . xxmaj the acting is really cheesy in some parts , and the \" action \" scenes are completely laughable . xxmaj you 'll burst out laughing at some parts which was a positive for me because it kept me mildly entertained . xxmaj the plot is some girl has a curse on her which causes her to vomit snakes so some shaman has to get her to xxmaj los xxmaj angeles , there are also two girls trying to smuggle drugs there and a few other people that are unimportant to the plot , not that there really is a plot at all . xxmaj don't expect anything from this movie and do n't listen to the cover , there are not 100 passengers and 3,000 vipers , there are 10 passengers and 20 random snakes . \n",
       " \n",
       "  xxmaj as for the xxup dvd , there is a trailer which is almost as laughable as the film , a blooper reel which is just one shot over and over of one actor trying to say train , and the deleted scenes are really pointless , if they were n't good enough to stay in this movie they must be pretty bad . xxmaj there is also a really bad making of featurette which does n't really show much at all except that the people involved with this movie were kind of idiots . i ca n't recommend it unless you want a really bad movie that you can laugh at with friends . i give it 2 kitty cats out of 5 .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: C:\\Users\\dmber\\.fastai\\data\\imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(49848, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(49848, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=49848, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x000002821FABCBF8>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=WindowsPath('C:/Users/dmber/.fastai/data/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0, MixedPrecision\n",
       "learn: ...\n",
       "loss_scale: 65536\n",
       "max_noskip: 1000\n",
       "dynamic: True\n",
       "clip: None\n",
       "flat_master: False\n",
       "max_scale: 16777216], layer_groups=[Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Embedding(49848, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(49848, 400, padding_idx=1)\n",
       "  )\n",
       "  (2): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=49848, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Loading the model if we need to \n",
    "learn_lm.load('fit_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/dmber/.fastai/data/imdb/imdb.vocab'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/lm_databunch'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/models'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/README'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/test'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/tmp_clas'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/tmp_lm'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/train'),\n",
       " WindowsPath('C:/Users/dmber/.fastai/data/imdb/unsup')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you're wondering where the model exist, it exist within this path under /models\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Fine-Tune we will unfreeze all the weights to adjust all parameters\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding optimal learning rate once more\n",
    "learn_lm.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 39:11 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.879189</td>\n",
       "      <td>3.852729</td>\n",
       "      <td>0.313177</td>\n",
       "      <td>09:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.774131</td>\n",
       "      <td>3.775172</td>\n",
       "      <td>0.323474</td>\n",
       "      <td>09:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.629352</td>\n",
       "      <td>3.716798</td>\n",
       "      <td>0.330592</td>\n",
       "      <td>09:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.500514</td>\n",
       "      <td>3.710171</td>\n",
       "      <td>0.331989</td>\n",
       "      <td>09:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.fit_one_cycle(4, 1e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the new trained model\n",
    "learn_lm.save('fine_tuned1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you would like to load in the pre-trained model that's not fine-tuned run the following cell\n",
    "# learn_lm.load('fine_tuned1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing out encoder\n",
    "# remember that an LSTM-RNN is composed of an encoder network and decoder network\n",
    "encoder = learn_lm.model[0].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i liked this movie because it was kind of a very good movie . It was about a family that is still living in the United States after the war . The family is very well concerned and they do n't\n",
      "i liked this movie because i loved it . But the plot did n't really make an impact . They could have used a joke as a camera . But i did . The story was good , and the story\n"
     ]
    }
   ],
   "source": [
    "# Testing our new model now to generate text\n",
    "TEXT = 'i liked this movie because'\n",
    "fr = generate_fake_review(learn_lm, TEXT)\n",
    "print(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie was the worst movie i 've ever seen . i laughed so hard i got to laugh my head off . the acting is terrible . and the characters are all stupid . and the movie is boring . i mean\n"
     ]
    }
   ],
   "source": [
    "TEXT = 'this movie was'\n",
    "fr = generate_fake_review(learn_lm, TEXT, n_sentences_=1)\n",
    "print(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 10:33 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.566832</td>\n",
       "      <td>3.715657</td>\n",
       "      <td>0.331932</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training for one more time\n",
    "learn_lm.fit_one_cycle(1, 1e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
