{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Movie Reviews (Using Naive Bayes, Logistic Regression, and Ngrams)\n",
    "The purpose of this notebook is to go over Naives Bayes, Logistic Regression, and Ngrams for sentiment classification. Using sklearn and fastai. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as sklearn_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always good to start working on a sample of your data before you use the full dataset - this allows for quicker computations as you debug and get your code working. \n",
    "\n",
    "We will be using IMBD, which already has a sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m  \u001b[0mURLs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mURLs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Global constants for dataset and model URLs.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mLOCAL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://s3.amazonaws.com/fast-ai-'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS3_IMAGE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3}imageclas/'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS3_IMAGELOC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3}imagelocal/'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS3_NLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3}nlp/'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS3_COCO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3}coco/'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS3_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3}modelzoo/'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mCOCO_SAMPLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3_COCO}coco_sample'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mCOCO_TINY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}coco_tiny'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mMNIST_SAMPLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}mnist_sample'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mMNIST_TINY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}mnist_tiny'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mIMDB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3_NLP}imdb'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mIMDB_SAMPLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}imdb_sample'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mHUMAN_NUMBERS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}human_numbers'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mADULT_SAMPLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}adult_sample'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mML_SAMPLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}movie_lens_sample'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mPLANET_SAMPLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}planet_sample'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mBIWI_SAMPLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}biwi_sample'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mPLANET_TINY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}planet_tiny'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mCIFAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}cifar10'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mWT103\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3_MODEL}wt103'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mWT103_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3_MODEL}wt103-1'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# kaggle competitions download dogs-vs-cats -p {DOGS.absolute()}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mDOGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}dogscats'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mPETS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3_IMAGE}oxford-iiit-pet'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mMNIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3_IMAGE}mnist_png'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mCAMVID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3_IMAGELOC}camvid'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mCAMVID_TINY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{URL}camvid_tiny'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mBIWI_HEAD_POSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{S3_IMAGELOC}biwi_head_pose\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mLSUN_BEDROOMS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{S3_IMAGE}bedroom'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/lib/python3.6/site-packages/fastai/datasets.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checkign the datasets we have\n",
    "?? URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb_sample')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In our case we will use the IMDB Sample\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "# pointing to the path\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  is_valid\n",
       "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
       "1  positive  This is a extremely well-made film. The acting...     False\n",
       "2  negative  Every once in a long while a movie will come a...     False\n",
       "3  positive  Name just says it all. I watched this movie wi...     False\n",
       "4  negative  This movie succeeds at being one of the most u...     False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get a sense of what the data looks like - we will not use this for our model\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['texts.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the path directy - this is what the file is called with our dataset - IMDB sample\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m  \u001b[0mTextList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Basic `ItemList` for text data.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/lib/python3.6/site-packages/fastai/text/data.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We will be using TextLit from FastAI library to process our data\n",
    "? TextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting movie reviews - using TextList \n",
    "\n",
    "\"\"\"\n",
    "path: path to our dataset\n",
    "'texts.csv': name of our dataset\n",
    "cols='text': column text is the text we want to process\n",
    "\n",
    "cols2: isValid\n",
    "cols0: Our label - we will label from dataframe therefore we point to col0\n",
    "\"\"\"\n",
    "\n",
    "movie_reviews = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
    "                         .split_from_df(col=2)\n",
    "                         .label_from_df(cols=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring what our data looks like\n",
    "A good first step for any data problem is to explore the data and get a sense of what it looks like. In this case we are looking at movie reviews, which have been labeled as \"Positive\" or \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj this very funny xxmaj british comedy shows what might happen if a section of xxmaj london , in this case xxmaj xxunk , were to xxunk itself independent from the rest of the xxup uk and its laws , xxunk & post - war xxunk . xxmaj merry xxunk is what would happen . \n",
       " \n",
       "  xxmaj the explosion of a wartime bomb leads to the xxunk of ancient xxunk which show that xxmaj xxunk was xxunk to the xxmaj xxunk of xxmaj xxunk xxunk ago , a small historical xxunk long since forgotten . xxmaj to the new xxmaj xxunk , however , this is an unexpected opportunity to live as they please , free from any xxunk from xxmaj xxunk . \n",
       " \n",
       "  xxmaj stanley xxmaj xxunk is excellent as the minor city xxunk who suddenly finds himself leading one of the world 's xxunk xxunk . xxmaj xxunk xxmaj margaret xxmaj xxunk is a delight as the history professor who sides with xxmaj xxunk . xxmaj others in the stand - out cast include xxmaj xxunk xxmaj xxunk , xxmaj paul xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk & xxmaj sir xxmaj michael xxmaj xxunk . \n",
       " \n",
       "  xxmaj welcome to xxmaj xxunk !, Category positive)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking first review from valid \n",
    "movie_reviews.valid.x[0], movie_reviews.valid.y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, a **token** is the basic unit of processing (what the tokens are depends on the application and your choices). Here, the tokens mostly correspond to words or punctuation, as well as several special tokens, corresponding to unknown words, captialization, etc. \n",
    "\n",
    "From looking at our example - this is post-processed data that has been **tokenized**. You will see that some words have ``xx``. Here is a dictionary explaining what they mean. \n",
    "\n",
    "* ```UNK```: Is for an unknown word (one that isn't present in the current vocabulary)\n",
    "* ```PAD```: Is the token used for padding, if we need to regroup several texts of different lengths in a batch\n",
    "* ```BOS```: Represents the beginning of a text in your dataset\n",
    "* ```FLD```: Is used if you set ```mark_fields=True``` is your ```TokenizeProcessor``` to seperate the different fields of texts (if your text are loaded from several different columns in a dataframe)\n",
    "* ```TK_MAJ```: Is used to indicate the next word begins with a capital in the original text\n",
    "* ```TK_UP```: Is used to indicate the next word is written in all caps in the original text\n",
    "* ```TK_REP```: Is used to indicate the next character is repeated n timess in the original text \n",
    "* ```TK_WREP```: Is used to indicate the next word is repeated n times in the original text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```itos``` is integers to string. \n",
    "```stoi``` is string to integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010, 19159)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the shape of each?\n",
    "len(movie_reviews.vocab.itos), len(movie_reviews.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxfld',\n",
       " 'xxmaj',\n",
       " 'xxup',\n",
       " 'xxrep',\n",
       " 'xxwrep',\n",
       " 'the',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at tokens\n",
    "movie_reviews.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but', 'film', 'you', ')', 'on', '(', \"n't\", 'are', 'he', 'his']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.vocab.itos[30:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you're confused: ```movie_reviews.vocab.stoi``` is basically a dictionary which is our vocab. \n",
    "\n",
    "In this dictionary we contain the index and the token it represents. \n",
    "\n",
    "Therefore with ```movie_reviews.vocab.itos``` we can actually slice the list giving it the 'index' which will point to the string it represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1367"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look up the index for a word\n",
    "movie_reviews.vocab.stoi['planet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'planet'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to confirm this\n",
    "movie_reviews.vocab.itos[1367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial word: sex\n",
      "Index it points to: 433\n",
      "Token returned: sex\n"
     ]
    }
   ],
   "source": [
    "# checking where a word maps to \n",
    "word = 'sex'\n",
    "\n",
    "idx = movie_reviews.vocab.stoi[word] # getting index for that word\n",
    "token_returned = movie_reviews.vocab.itos[idx]\n",
    "\n",
    "print(f'Initial word: {word}')\n",
    "print(f'Index it points to: {idx}')\n",
    "print(f'Token returned: {token_returned}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Term-Document Matrix\n",
    "As covered in notebook 1, a term-document matrix represents a document as a \"bag of words\", that is, we don't keep track of the order of the words are in, just which words occur (and how often). \n",
    "\n",
    "In that notebook we used SkLearn's ```CountVectorizer```. In this notebook we will create our own. Why? \n",
    "* To understand what is happening 'underneath the hood'\n",
    "* To create something that will work with fastai TextList\n",
    "\n",
    "To create our term-document matrix, we first need to learn about **counters** and **sparse matrices** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counters\n",
    "Counters are a userful Python object. Below is how they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter([4, 2, 8, 8, 4, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 2, 2: 1, 8: 3})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what the counter object does is that it takes a list of object 'ints' in our case and **counts** how many times they appear in that list. Returning that count as a dictionary.\n",
    "\n",
    "If you look about 4: 2 means that 4 appears twice in the list. 8: 3 means that 8 appears 3 times in the list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Matrices (SciPy)\n",
    "Even though we have reduced over 19,000 words down to 6,000, that is still a lot. Most tokens don't appear in most reviews. We want to take advantage of this by storing our data as a **sparse matrix**.\n",
    "\n",
    "A matrix with lots of zeros is called **sparse** (opposite of **dense**). For sparse matrices, you can save a lot of memory by only storing the non-zero values. \n",
    "\n",
    "There are the most common sparse storage formats: \n",
    "\n",
    "* Coordinate-wise (scipy calls COO)\n",
    "* Compressed sparse row (CSR)\n",
    "* Compressed sparse columns (CSC)\n",
    "\n",
    "A class of matrices is generally called sparse if the number of non-zero elements is proportional to the number of rows (or columns) instead of being proportional to the product rows x columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Version of ```CountVectorizer```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each document we use counter on will be:\n",
    "doc = movie_reviews.valid.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text xxbos xxmaj this very funny xxmaj british comedy shows what might happen if a section of xxmaj london , in this case xxmaj xxunk , were to xxunk itself independent from the rest of the xxup uk and its laws , xxunk & post - war xxunk . xxmaj merry xxunk is what would happen . \n",
       "\n",
       " xxmaj the explosion of a wartime bomb leads to the xxunk of ancient xxunk which show that xxmaj xxunk was xxunk to the xxmaj xxunk of xxmaj xxunk xxunk ago , a small historical xxunk long since forgotten . xxmaj to the new xxmaj xxunk , however , this is an unexpected opportunity to live as they please , free from any xxunk from xxmaj xxunk . \n",
       "\n",
       " xxmaj stanley xxmaj xxunk is excellent as the minor city xxunk who suddenly finds himself leading one of the world 's xxunk xxunk . xxmaj xxunk xxmaj margaret xxmaj xxunk is a delight as the history professor who sides with xxmaj xxunk . xxmaj others in the stand - out cast include xxmaj xxunk xxmaj xxunk , xxmaj paul xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk & xxmaj sir xxmaj michael xxmaj xxunk . \n",
       "\n",
       " xxmaj welcome to xxmaj xxunk !"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  4, 20, 70, ..., 14,  4,  0, 51])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_doc_matrix(label_list, vocab_len):\n",
    "    j_indices = []\n",
    "    indptr = []\n",
    "    values = []\n",
    "    indptr.append(0) # initiate counter\n",
    "    \n",
    "    for i, doc in enumerate(label_list):\n",
    "        feature_counter = Counter(doc.data) # grab each word - we grab the index\n",
    "        j_indices.extend(feature_counter.keys()) # place word index in j index\n",
    "        values.extend(feature_counter.values()) # place word in values list\n",
    "        indptr.append(len(j_indices))\n",
    "        \n",
    "    # Returning sparse matrix (values, j_indices, indptr)\n",
    "    return scipy.sparse.csr_matrix((values, j_indices, indptr),\n",
    "                                    shape=(len(indptr) - 1, vocab_len),\n",
    "                                    dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using our countvectorizer\n",
    "label_list = movie_reviews.valid.x # all the documents\n",
    "vocab_len = len(movie_reviews.vocab.itos) # list of all words - grabbing length\n",
    "\n",
    "val_term_doc = get_term_doc_matrix(label_list, vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 6010)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 200 reviews, 6010 words - sparsely represented\n",
    "val_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our train term-doc\n",
    "label_list = movie_reviews.train.x\n",
    "vocab_len = vocab_len # same thing\n",
    "\n",
    "train_term_doc = get_term_doc_matrix(label_list, vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 6010)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 800 reviews, 6010 words - sparsely represented\n",
    "train_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[32,  0,  1,  0, ...,  0,  0, 10,  7],\n",
       "        [ 9,  0,  1,  0, ...,  0,  0,  7,  8],\n",
       "        [ 6,  0,  1,  0, ...,  0,  0, 12, 12],\n",
       "        [78,  0,  1,  0, ...,  0,  0, 44, 23],\n",
       "        ...,\n",
       "        [ 8,  0,  1,  0, ...,  0,  0,  8,  8],\n",
       "        [43,  0,  1,  0, ...,  1,  0, 25, 24],\n",
       "        [ 7,  0,  1,  0, ...,  0,  0,  9,  9],\n",
       "        [19,  0,  1,  0, ...,  0,  0,  5,  9]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check these matrices out\n",
    "# We need to convert todense()\n",
    "val_term_doc.todense()[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 8,  0,  1,  0, ...,  0,  0,  2,  3],\n",
       "        [22,  0,  1,  0, ...,  0,  0, 27, 19],\n",
       "        [ 4,  0,  1,  0, ...,  0,  0,  5, 14],\n",
       "        [13,  0,  1,  0, ...,  0,  0, 16,  7],\n",
       "        ...,\n",
       "        [ 4,  0,  1,  0, ...,  0,  0, 19,  7],\n",
       "        [42,  0,  1,  0, ...,  0,  0, 30, 15],\n",
       "        [18,  0,  1,  0, ...,  0,  0, 15, 11],\n",
       "        [20,  0,  1,  0, ...,  0,  0, 10,  4]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_term_doc.todense()[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  4, 20, 70, ..., 14,  4,  0, 51])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj this very funny xxmaj british comedy shows what might happen if a section of xxmaj london , in this case xxmaj xxunk , were to xxunk itself independent from the rest of the xxup uk and its laws , xxunk & post - war xxunk . xxmaj merry xxunk is what would happen . \\n\\n xxmaj the explosion of a wartime bomb leads to the xxunk of ancient xxunk which show that xxmaj xxunk was xxunk to the xxmaj xxunk of xxmaj xxunk xxunk ago , a small historical xxunk long since forgotten . xxmaj to the new xxmaj xxunk , however , this is an unexpected opportunity to live as they please , free from any xxunk from xxmaj xxunk . \\n\\n xxmaj stanley xxmaj xxunk is excellent as the minor city xxunk who suddenly finds himself leading one of the world 's xxunk xxunk . xxmaj xxunk xxmaj margaret xxmaj xxunk is a delight as the history professor who sides with xxmaj xxunk . xxmaj others in the stand - out cast include xxmaj xxunk xxmaj xxunk , xxmaj paul xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk & xxmaj sir xxmaj michael xxmaj xxunk . \\n\\n xxmaj welcome to xxmaj xxunk !\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun Thought 1\n",
    "A function $f(x_{[i,n]}) = \\hat{y}$ where $x_{[i,n]}$ represents $x_{i}$ features up to $x_{n}$ and $\\hat{y}$ represents the output which is most cases is a de-coherent distributed form. \n",
    "\n",
    "In a high level: when plotting $x_{[i,n]}$ (in $m$ dimensional space) you form a **map** to it's de-coherent distribution which is a point in the **whole distribution** of the system you are mapping. \n",
    " \n",
    "Each feature $x$ must be numerical in nature; thus when 'processing text' we use embeddings or other numerical representations of the text to process. \n",
    "\n",
    "An example to get this notion accross think of this scenario: You want to go play golf but you want to know if it's even a good day to play golf. Are the 'weather' *parameters* good? And *good* in a way that those *parameters* when plugged into $f(x)$ fall within a satisfactory de-coherent distribution. \n",
    "\n",
    "Therefore, for a *system* to learn this **whole distribution** to *map* $x_{[i,n]}$ to $\\hat{y}$ we need to run **MANY** experiments. **Many** being defined as enough to hold *Statistical value*. \n",
    "\n",
    "Therefore, in *Machine Learning* we need a lot of data to make this distribution to *Learn* this distribution. \n",
    "\n",
    "This same principle is the core of common *experimental method* when conducting research. \n",
    "\n",
    "# Naives Bayes\n",
    "This algorithm makes an assumption as all the variables in the dataset is *Naive* or **not correlated to each other**. This algorithm is usually used to get base accuracy of the dataset.\n",
    "\n",
    "$$P(c | x) = \\frac{P(x|c)P(c)}{P(x)}$$\n",
    "\n",
    "Where:\n",
    "* $P(c|x)$ is the **posterior probability** of *class c* given *predictor x (features)*\n",
    "* $P(c)$ is the probability of *class*\n",
    "* $P(x|c)$ is the **likelihood** which is the probability of predictor given class\n",
    "* $P(x)$ is the **prior probability** of predictor\n",
    "\n",
    "### Advantages of Naives Bayes\n",
    "* It is easy and fast to predict the class of the test dataset. It also performs well in multi-class prediction\n",
    "* When assumption of independence holds, a Naive Bayes classifier performs better compared to other models like Logistic Regression and you need less training data\n",
    "* It performs well in case of categorical input variables compared to numerical variables. For numerical variable, a normal distribution is assumed (bell curve, which is a strong assumption)\n",
    "\n",
    "### Disadvantages of Naives Bayes\n",
    "* If categorical variable has a category (in test dataset), which has not observed in training data set, then the model will assign 0 (zero) probability and will be unable to make a prediction. This is often known as **zero frequency**. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called *Laplace estimation*\n",
    "* On the other side Naive Bayes is also known as a bad estimator, so the probability outputs are not to be taken too seriously\n",
    "* Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent \n",
    "\n",
    "We define the **log-count ratio $r$** for each word $f$:\n",
    "$$ r = log\\frac{\\text{ratio of feature} f \\text{in positive documents}}{\\text{ratio of feature} f \\text{in negative documents}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_term_doc # sparse matrix\n",
    "y = movie_reviews.train.y # labels for train dataset\n",
    "val_y = movie_reviews.valid.y # labels for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing int values of our classes from y\n",
    "positive = y.c2i['positive']\n",
    "negative = y.c2i['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing frequency of positive and negative for each word in our vocabulary\n",
    "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0))) # summing up positives\n",
    "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0))) # summing up negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010, 6010)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p1), len(p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6468,     0,   383,     0, 10267,   674,    57,     0,  5260,  4195], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7153,     0,   417,     0, 10741,   908,    53,     1,  6150,  5147], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing vocab\n",
    "v = movie_reviews.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count: 29\n",
      "Negative count: 12\n"
     ]
    }
   ],
   "source": [
    "# How often does a word appear in positive & negative reviews?\n",
    "def count_word_pos_neg(word):\n",
    "    return p1[v.stoi[word]], p0[v.stoi[word]]\n",
    "\n",
    "p,n = count_word_pos_neg('loved')\n",
    "print(f'Positive count: {p}')\n",
    "print(f'Negative count: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count: 6468\n",
      "Negative count: 7153\n"
     ]
    }
   ],
   "source": [
    "p,n = count_word_pos_neg('lust')\n",
    "print(f'Positive count: {p}')\n",
    "print(f'Negative count: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing a review\n",
    "def grab_random_review(review_idxs_list_, d_set=\"train\"):\n",
    "    \"\"\"\n",
    "    This function will take a list of review indexs (positive or negative) and will return \n",
    "    \"\"\"\n",
    "    random_idx = random.choice(review_idxs_list_)\n",
    "    \n",
    "    if d_set==\"train\":\n",
    "        review_ = movie_reviews.train.x[random_idx]\n",
    "    elif d_set==\"valid\":\n",
    "        review_ = movie_reviews.valid.x[random_idx]\n",
    "    else:\n",
    "        return \"Wrong dataset typed in\"\n",
    "    \n",
    "    return review_.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviewing review from train dataset\n",
    "def grab_list_postive_negative_for_word(word):\n",
    "    \"\"\"\n",
    "    This takes in a word and will find all positive and negative reviews that contain that specific word. Therefore this function will return a negative and positive list of review indexes\n",
    "    \n",
    "    RETURNS:\n",
    "        positive_review_list_idxs, negative_review_list_idxs\n",
    "    \"\"\"\n",
    "    # grab word index\n",
    "    word_idx = v.stoi[word]\n",
    "    \n",
    "    # positive\n",
    "    a_p = np.argwhere((x[:,word_idx] > 0))[:,0]\n",
    "    b_p = np.argwhere(y.items==positive)[:,0] \n",
    "    sets_p = set(a).intersection(set(b))\n",
    "    \n",
    "    # positive\n",
    "    a_n = np.argwhere((x[:,word_idx] > 0))[:,0]\n",
    "    b_n = np.argwhere(y.items==negative)[:,0] \n",
    "    sets_n = set(a).intersection(set(b))\n",
    "    \n",
    "    # our list of indexes\n",
    "    idxs_found_p = [] # positive list\n",
    "    idxs_found_n = [] # negative list\n",
    "    \n",
    "    # grabbing possitive\n",
    "    for _ in range(sets_p.__len__()):\n",
    "        idx = sets_p.pop() # grabbing each index\n",
    "        idxs_found_p.append(idx) # adding to our list\n",
    "        \n",
    "    # grabbing negative\n",
    "    for _ in range(sets_n.__len__()):\n",
    "        idx = sets_n.pop() # grabbing each index\n",
    "        idxs_found_n.append(idx) # adding to our list\n",
    "        \n",
    "    return idxs_found_p, idxs_found_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Review for word: love\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "xxbos xxmaj there are numerous films relating to xxup xxunk , but xxmaj mother xxmaj night is quite xxunk among them : xxmaj in this film , we are introduced to xxmaj howard xxmaj campbell ( xxmaj nolte ) , an xxmaj american living in xxmaj berlin and married to a xxmaj german , xxmaj xxunk xxmaj xxunk ( xxmaj lee ) , who decides to accept the role of a spy : xxmaj more specifically , a xxup cia agent xxmaj major xxmaj xxunk ( xxmaj goodman ) recruits xxmaj campbell who becomes a xxmaj nazi xxunk in order to enter the highest xxunk of the xxmaj hitler xxunk . xxmaj however , the deal is that the xxup us xxmaj government will never xxunk xxmaj campbell 's role in the war for national security reasons , and so xxmaj campbell becomes a hated figure across the xxup us . xxmaj after the war , he tries to xxunk his identity , but the past comes back and xxunk him . xxmaj his only \" friend \" is xxmaj xxunk , but even he can not do much for the xxunk of events that fall upon poor xxmaj campbell ... \n",
      "\n",
      " xxmaj the story is deeply touching , as we watch the tragedy of xxmaj campbell who although a great patriot , is treated by xxunk by everybody who xxunk him . xxmaj not only that , but he also gradually realizes that even the persons who are most close to him , have many xxunk of their own . xxmaj vonnegut provides us with a moving atmosphere , with xxmaj campbell 's despair building up and almost choking the viewer . \n",
      "\n",
      " xxmaj nolte plays the role of his life , in my opinion ; he is even better than in \" xxmaj xxunk \" , although in both roles he plays tragic figures who are destined to self - destruction . xxmaj xxunk xxmaj lee is also excellent , and the same can be said for the whole cast in general . \n",
      "\n",
      " i have n't read the book , so i can not xxunk how the film compares to it . xxmaj in any case , this is something of no importance here : xxmaj my xxunk is upon the film per xxunk , and the film xxunk deserves a 9 / 10 .\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's grab a random review\n",
    "word = \"love\"\n",
    "\n",
    "# positive review\n",
    "idxs_p, _ = grab_list_postive_negative_for_word(word)\n",
    "review = grab_random_review(idxs_p)\n",
    "\n",
    "print(f'Positive Review for word: {word}\\n')\n",
    "print('-----------------------------------------------------------------------')\n",
    "print(review)\n",
    "print('-----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
    "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))\n",
    "\n",
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.015811,  0.084839,  0.      ,  0.084839, ...,  1.471133, -1.301455, -1.301455, -1.301455])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.log(pr1/pr0); r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab most associated with positive and negative reviews - idxs\n",
    "biggest = np.argpartition(r, -10)[-10:]\n",
    "smallest = np.argpartition(r, 10)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words: \n",
      "['han', 'jabba', 'davies', 'gilliam', 'jimmy', 'felix', 'biko', 'fanfan', 'astaire', 'noir']\n",
      "\n",
      "Negative words: \n",
      "['vargas', 'naschy', 'worst', 'dog', 'porn', 'crater', 'crap', 'disappointment', 'soderbergh', 'fuqua']\n"
     ]
    }
   ],
   "source": [
    "# most positive words\n",
    "p_words = [v.itos[k] for k in biggest]\n",
    "print(f'Positive words: \\n{p_words}\\n')\n",
    "\n",
    "# most negative words\n",
    "n_words = [v.itos[k] for k in smallest]\n",
    "print(f'Negative words: \\n{n_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: (383, 417)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.645"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Naives Bayes\n",
    "ratio = ((y.items==positive).sum(), (y.items==negative).sum())\n",
    "print(f'ratio: {ratio}')\n",
    "\n",
    "b = np.log(ratio[0] / ratio[1])\n",
    "\n",
    "preds = (val_term_doc @ r + b) > 0\n",
    "\n",
    "# measuring our accuracy\n",
    "(preds == val_y.items).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from using a **sample** of our data we got a **64%** accuracy\n",
    "\n",
    "# Switching to full data set\n",
    "Now we will do the same approach as above on our entire data\n",
    "\n",
    "## Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/test'),\n",
       " PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/tmp_clas'),\n",
       " PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/unsup'),\n",
       " PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/README'),\n",
       " PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/tmp_lm'),\n",
       " PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/train')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/train/neg'),\n",
       " PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/train/pos'),\n",
       " PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/train/unsupBow.feat'),\n",
       " PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb/train/labeledBow.feat')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkign train folder\n",
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our dataset with fastai\n",
    "reviews_full = (TextList.from_folder(path)\n",
    "                        .split_by_folder(valid='test')\n",
    "                        .label_from_folder(classes=['neg', 'pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_full.train), len(reviews_full.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing our vocab\n",
    "v = reviews_full.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38458, 118247)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v.itos), len(v.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people', 'will', 'other', 'also', 'into']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grabbing a few words\n",
    "v.itos[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m  \u001b[0mget_term_doc_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Desktop/MadGeniusLearning/FASTAI_NLP/<ipython-input-19-7ae55a3c6a90>\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? get_term_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our term documents\n",
    "val_term_doc = get_term_doc_matrix(reviews_full.valid.x, len(reviews_full.vocab.itos))\n",
    "trn_term_doc = get_term_doc_matrix(reviews_full.train.x, len(reviews_full.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our documents\n",
    "scipy.sparse.save_npz('trn_term_doc.npz', trn_term_doc)\n",
    "scipy.sparse.save_npz('val_term_doc.npz', val_term_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_term_doc.npz val_term_doc.npz\n"
     ]
    }
   ],
   "source": [
    "# Moving them into a new data directory\n",
    "!mkdir data\n",
    "!mv trn_term_doc.npz ./data; mv val_term_doc.npz ./data\n",
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How to load - don't need to run now\n",
    "# trn_term_doc = scipy.sparse.load_npz('./data/trn_term_doc.npz')\n",
    "# val_term_doc = scipy.sparse.load_npz('./data/val_term_doc.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0,  0,  1,  0, ...,  0,  0,  2,  2],\n",
       "        [ 0,  0,  1,  0, ...,  0,  0, 12,  5],\n",
       "        [ 0,  0,  1,  0, ...,  0,  0,  4,  9],\n",
       "        [ 4,  0,  1,  0, ...,  1,  0, 19, 23],\n",
       "        ...,\n",
       "        [ 3,  0,  1,  0, ...,  0,  0, 10,  7],\n",
       "        [ 2,  0,  1,  0, ...,  0,  0,  4,  5],\n",
       "        [ 0,  0,  1,  0, ...,  0,  0,  9,  7],\n",
       "        [ 2,  0,  1,  0, ...,  0,  0, 16,  7]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking our matrix\n",
    "trn_term_doc.todense()[:10,:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc # our matrix\n",
    "y = reviews_full.train.y # our true labels\n",
    "\n",
    "val_y = reviews_full.valid.y.items # our validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x38458 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3716653 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gettting labels\n",
    "positive = y.c2i['pos']\n",
    "negative = y.c2i['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing p1, p0\n",
    "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))\n",
    "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr1 / pr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.log((y.items==positive).mean() / (y.items==negative).mean()); b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (val_term_doc @ r + b) > 0 # helps avoid zeros in our matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8086"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Now we will use **logistic regression** to tackle the same problem. \n",
    "\n",
    "Before moving forward: Let's learn more about **logistic regression**. \n",
    "\n",
    "#### Outcome\n",
    "In logistic regression, the outcome (dependent variable) has only a limited number of possible values *usually between 0 and 1*. \n",
    "\n",
    "#### The dependent variable\n",
    "Logistic regression is used when the response variable is categorical in nature. For instance: yes/no, true/false, red/green/blue, etc. \n",
    "\n",
    "#### Coefficient interpretation\n",
    "In logistic regression, the coefficient interpretation depends on the family (binomial, Possion, etc) and link (log, logit, inverse-log, etc) you use. \n",
    "\n",
    "#### Error minimization technique\n",
    "Logistic regression uses maximum likelihood method to arrive at the optimal solution. \n",
    "\n",
    "Using logistic loss function causes large errors to be penalized to an asymptotically constant. \n",
    "\n",
    "Below is how we can fit logistic regression where the features are the unigrams\n",
    "\n",
    "## What is a unigram and ngram?\n",
    "A **Unigram** just represents a single word, whereas an **ngram** represents a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88272"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x, y.items.astype(int))\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean() # printing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88544"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary approach - does the word exist or not?\n",
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(trn_term_doc.sign(), y.items.astype(int))\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean() # printing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigam with NB features\n",
    "Our next model is a version of logistic regression with Naive Bayes feature. For every document we compute binarized features as shown above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment. \n",
    "\n",
    "## ngrams\n",
    "An n-gram is a contigous sequence of n items (where the items can be characters, syllables, or words). A 1-gram is a unigram, a 2-gram is a bigram, and a 3-gram is a trigram. \n",
    "\n",
    "Here, we are referring to sequence of words. So examples of bigrams include \"the dog\", \"said that\", and \"can't you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/diegomedina-bernal/.fastai/data/imdb_sample/texts.csv')]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "path = untar_data(URLs.IMDB_SAMPLE) # sample\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to TextList object\n",
    "movie_reviews = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
    "                         .split_from_df(col=2)\n",
    "                         .label_from_df(cols=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing our vocab\n",
    "v = movie_reviews.vocab.itos # returns list with each word\n",
    "\n",
    "vocab_len = len(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram definition\n",
    "min_n = 1\n",
    "max_n = 3\n",
    "\n",
    "j_indices = []\n",
    "indptr = []\n",
    "values = []\n",
    "indptr.append(0)\n",
    "num_tokens = vocab_len\n",
    "\n",
    "itongram = dict()\n",
    "ngramtoi = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through sequence of words to create ngrams\n",
    "for i, doc in enumerate(movie_reviews.train.x):\n",
    "    feature_counter = Counter(doc.data)\n",
    "    j_indices.extend(feature_counter.keys())\n",
    "    values.extend(feature_counter.values())\n",
    "    \n",
    "    this_doc_ngrams = list()\n",
    "    \n",
    "    m = 0\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for k in range(vocab_len - n + 1):\n",
    "            ngram = doc.data[k: k + n]\n",
    "            if str(ngram) not in ngramtoi:\n",
    "                if len(ngram)==1:\n",
    "                    num = ngram[0]\n",
    "                    ngramtoi[str(ngram)] = num\n",
    "                    itongram[num] = ngram\n",
    "                else:\n",
    "                    ngramtoi[str(ngram)] = num_tokens\n",
    "                    itongram[num_tokens] = ngram\n",
    "                    num_tokens += 1\n",
    "            this_doc_ngrams.append(ngramtoi[str(ngram)])\n",
    "            m += 1\n",
    "            \n",
    "    ngram_counters = Counter(this_doc_ngrams)\n",
    "    j_indices.extend(ngram_counters.keys())\n",
    "    values.extend(ngram_counters.values())\n",
    "    indptr.append(len(j_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our matrix\n",
    "train_ngram_doc_matrix = scipy.sparse.csr_matrix((values, j_indices, indptr),\n",
    "                                                shape=(len(indptr) - 1, len(ngramtoi)),\n",
    "                                                dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x260428 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 678936 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[16,  0,  2,  0, ...,  0,  0,  4,  6],\n",
       "        [44,  0,  2,  0, ...,  0,  0, 54, 40],\n",
       "        [ 8,  0,  2,  0, ...,  0,  0, 10, 30],\n",
       "        [26,  0,  2,  0, ...,  0,  0, 32, 16],\n",
       "        ...,\n",
       "        [ 8,  0,  2,  0, ...,  0,  0, 38, 16],\n",
       "        [84,  0,  2,  0, ...,  0,  0, 60, 32],\n",
       "        [36,  0,  2,  0, ...,  0,  0, 30, 24],\n",
       "        [40,  0,  2,  0, ...,  0,  0, 20, 10]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_doc_matrix.todense()[:10, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on ```train_ngram_doc_matrix```. Here we have a sparse matrix of size: (800, 260428), where 800 represents 800 reviews, 260428 represents that many ngrams (bi-grams, tri-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating validation matrix\n",
    "j_indices = []\n",
    "indptr = []\n",
    "values = []\n",
    "indptr.append(0)\n",
    "\n",
    "for i, doc in enumerate(movie_reviews.valid.x):\n",
    "    feature_counter = Counter(doc.data)\n",
    "    j_indices.extend(feature_counter.keys())\n",
    "    values.extend(feature_counter.values())\n",
    "    this_doc_ngrams = list()\n",
    "    \n",
    "    m = 0\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for k in range(vocab_len - n + 1):\n",
    "            ngram = doc.data[k: k+n]\n",
    "            if str(ngram) in ngramtoi:\n",
    "                this_doc_ngrams.append(ngramtoi[str(ngram)])\n",
    "            m += 1\n",
    "            \n",
    "    ngram_counter = Counter(this_doc_ngrams)\n",
    "    j_indices.extend(ngram_counter.keys())\n",
    "    values.extend(ngram_counter.values())\n",
    "    indptr.append(len(j_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ngram_doc_matrix = scipy.sparse.csr_matrix((values, j_indices, indptr),\n",
    "                                                shape=(len(indptr) - 1, len(ngramtoi)),\n",
    "                                                dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200x260428 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 121595 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ngram_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x260428 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 678936 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving our data\n",
    "scipy.sparse.save_npz('./data/train_ngram_matrix.npz', train_ngram_doc_matrix)\n",
    "scipy.sparse.save_npz('./data/valid_ngram_matrix.npz', valid_ngram_doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_ngram_doc_matrix\n",
    "y = movie_reviews.train.y\n",
    "\n",
    "positive = y.c2i['positive']\n",
    "negative = y.c2i['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x260428 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 678936 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 260428\n",
    "\n",
    "pos = (y.items == positive)[:k]\n",
    "neg = (y.items == negative)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = x[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = [o == positive for o in movie_reviews.valid.y.items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.squeeze(np.array(xx[neg].sum(0)))\n",
    "p1 = np.squeeze(np.array(xx[pos].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
    "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr1/pr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.log((y.items==positive).mean() / (y.items==negative).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds = valid_ngram_doc_matrix @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "\n",
    "(preds==valid_labels).mean() # our accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr = CountVectorizer(ngram_range=(1,3), preprocessor=noop, tokenizer=noop, max_features=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=800000, min_df=1,\n",
       "                ngram_range=(1, 3), preprocessor=<function noop at 0x137c63620>,\n",
       "                stop_words=None, strip_accents=None,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function noop at 0x137c63620>, vocabulary=None)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = movie_reviews.train.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = [[docs.vocab.itos[o] for o in doc.data] for doc in movie_reviews.train.x]\n",
    "valid_words = [[docs.vocab.itos[o] for o in doc.data] for doc in movie_reviews.valid.x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngram_doc = veczr.fit_transform(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x260427 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 565716 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200x260427 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 93547 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting valid matrix\n",
    "val_ngram_doc = veczr.transform(valid_words)\n",
    "val_ngram_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = veczr.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the room she',\n",
       " 'the room when',\n",
       " 'the room where',\n",
       " 'the rooms',\n",
       " 'the rooms are']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[200000:200005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegomedina-bernal/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/diegomedina-bernal/miniconda3/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.785"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running our model\n",
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(train_ngram_doc, y.items)\n",
    "preds = m.predict(val_ngram_doc)\n",
    "(preds.T == valid_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegomedina-bernal/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary\n",
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(train_ngram_doc.sign(), y.items)\n",
    "preds = m.predict(val_ngram_doc.sign())\n",
    "(preds.T == valid_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
