{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with NMF and SVD\n",
    "Topic Modeling is an unsupervised learning approach to figure out what is the best title for your document. Or Add title for ad copy, etc. \n",
    "\n",
    "The idea of matrix decompensition is taking one matrix and representing it as a few matrices. So example: Multiplying (matrix product) of these newly formed matrix will represent the original matrix. \n",
    "\n",
    "Example: We can decompose one large matrix into one tall thin matrix times one wide short matrix (possibly with a diagonal matrix in between). \n",
    "\n",
    "An example of this is called *latent semantic analysis (LSA)*. \n",
    "\n",
    "## Motivation \n",
    "Consider the most extreme case - reconstructing the matrix using an outer product of two vectors. Clearly, in most cases we won't be able to reconstruct the matrix exactly. But if we had one vector with the relative frequency of each vocabulary word out of the total word count, and one with the average number of words per document, then that outer product would be as close as we can get. \n",
    "\n",
    "Now consider increasing that matrices to two columns and two rows. The optimal decomposition would now be to cluster documents into two groups, each of which has as different distribution of words as possible to each other, but as similar as possible amongst the documents in the cluster. \n",
    "\n",
    "We will call those two groups \"Topics\". And we would cluster the words into two groups, based on those which most frequently appear in each of the topics. \n",
    "\n",
    "## Getting started\n",
    "We'll take a dataset of documents in several different categories, and find topics (consisting of groups of words) for them. Knowing the actual categoies helps us evaluate if the topic we find make sense. \n",
    "\n",
    "We will try this with two matrix factorizations: **Singular Value Decomposition (SVD)** and **Non-Negative Matrix Factorization**. \n",
    "\n",
    "### Singular Value Decomposition\n",
    "$$ A_{[m x n]} = U_{[m x n]} \\sum_{[r x r]}(V_{[n x r]})^T$$\n",
    "\n",
    "In which case $A$ in the **input data matrix** of size (m x n) where $m$ can be documents and $n$ can be terms. Therefor $m$ represents the rows and $n$ represents the columns in the matrix. \n",
    "\n",
    "So with **SVD** we take this initial input data matrix and we represent it as a *product* of three matrices. \n",
    "\n",
    "Therefor $U$ is the **left singular vectors** of shape (m x r) that is this matrix stores *left singular vectors* which can be represented as $m$ documents and $r$ concepts. \n",
    "\n",
    "our matrix $\\sum$ is called the **Singular values** matrix where: \n",
    "* r x r diagonal matrix (strenght of each 'concept')\n",
    "    * Contains zeros everywhere except diagonal which are singular values. These singular values are sorted in decreasing order. \n",
    "* r: rank of Matrix **A**\n",
    "\n",
    "Lastly we have $V$ which is called the **Right singular vectors**. The size of this matrix is (n x r) where $n$ represents terms and $r$ represents concepts. We also **transpose** this vector.\n",
    "\n",
    "#### SVD Properties\n",
    "It is always possible to decompose a real Matrix $A$ into $A = U \\sum V^T$ where:\n",
    "* $U, \\sum, V$: are unique\n",
    "* $U, V$: column orthonormal\n",
    "* $\\sum$: Diagonal\n",
    "    * Entries (Singular values) are positive and sorted in decreasing order\n",
    "    \n",
    "What is essentially happening with **SVD** is that we are able to **cluster** what a matrix represents via \"Concepts\" which are the right and left singular vectors with the singular values acting as weights or strength towards the concepts learned. \n",
    "\n",
    "These concepts aren't given to the model - they are simply learned via preference. \n",
    "\n",
    "### Non-Negative Matrix Factorization (NMF)\n",
    "For a matrix $V$ of dimensions (m x n) where each element $v_{ij} \\ge 0$, NMF decomposes it into two matrices $W$ and $H$ of dimensions (m x r) and (r x n) respectively, where each element $w_{ij} \\ge 0$ and $h_{ij} \\ge 0$ and $r < min(m,n)$\n",
    "\n",
    "such that: \n",
    "\n",
    "$$W * H = V$$\n",
    "\n",
    "The problem cannot be solved analytically so it is generally approximated numerically. \n",
    "\n",
    "$W, H$ are lower dimensional representations (latent space) of matrix $V$\n",
    "\n",
    "* So $V$ is decomposed into a tall, skinny matrix $W$ and a short, wide matrix $H$\n",
    "* The user can specify $r$, the inner dimension of $W$ and $H$, as long as it is $r < min(m,n)$\n",
    "* We see that each column of $V$, $v_i$, can be calcualted as:\n",
    "$$v_i = W * h_i$$\n",
    "* That is, each column of $V$ is equal to the sum of each column of $W$ after being weighted by its corresponding row $h_j$\n",
    "\n",
    "NMF is a way of reducing the dimensionality of our data into a **linear combination of bases**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will pick a few topics from our entire dataset\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes') # for each document\n",
    "\n",
    "# Creating train and test datasets\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2034,), (1353,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the shape of our datasets\n",
    "newsgroups_train.filenames.shape, newsgroups_test.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Showing one message\n",
    "print('\\n'.join(newsgroups_train.data[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comp.graphics', 'talk.religion.misc', 'sci.space'], dtype='<U18')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the targets for the top three?\n",
    "np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 0, 2, 0, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The target attribute is the integer index of the category\n",
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most 'cluster analysis' task we have to choose how many 'topics' or 'concepts' we want to create. We will represent this as ```num_topics, num_top_words```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be treated as a parameter to experiment with\n",
    "num_topics, num_top_words = 6, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "Some extreme common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called **stop words**. \n",
    "\n",
    "Generally stop words aren't used any more but we will do so in this case because we are using a small dataset and a simple model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing stop words\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "sorted(list(stop_words.ENGLISH_STOP_WORDS))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "Are the words below the same? \n",
    "\n",
    "*organize, organizes, organizing* \n",
    "\n",
    "*democracy, democratic, democratization*\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the words. \n",
    "\n",
    "Lemmatization uses the rules about a language. The resulting tokens are all actual words.\n",
    "\n",
    "Stemming is the poor-mans's lemmatization. Stemming is a crude heuristic that chops the end off of the word. The resulting token may not be actual words. Stemming is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/diegomedina-\n",
      "[nltk_data]     bernal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import stem\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer, Stemmer\n",
    "wnl = stem.WordNetLemmatizer()\n",
    "porter = stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing word list\n",
    "word_list = ['feet', 'foot', 'foots', 'footing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foot', 'foot', 'foot', 'footing']\n",
      "['feet', 'foot', 'foot', 'foot']\n"
     ]
    }
   ],
   "source": [
    "print([wnl.lemmatize(word) for word in word_list]) # lemmatization\n",
    "print([porter.stem(word) for word in word_list]) # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Original List of word: ['fly', 'flies', 'flying']\n",
      "Lemmativation of word list: ['fly', 'fly', 'flying']\n",
      "Stemming of word list: ['fli', 'fli', 'fli']\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Original List of word: ['organize', 'organizes', 'organizing']\n",
      "Lemmativation of word list: ['organize', 'organizes', 'organizing']\n",
      "Stemming of word list: ['organ', 'organ', 'organ']\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Original List of word: ['universe', 'university']\n",
      "Lemmativation of word list: ['universe', 'university']\n",
      "Stemming of word list: ['univers', 'univers']\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Trying more words\n",
    "def stem_lemm(word_list):\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(f'Original List of word: {word_list}')\n",
    "    print(f'Lemmativation of word list: {[wnl.lemmatize(word) for word in word_list]}')\n",
    "    print(f'Stemming of word list: {[porter.stem(word) for word in word_list]}')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    \n",
    "# Word lists \n",
    "list_one = ['fly', 'flies', 'flying']\n",
    "list_two = ['organize', 'organizes', 'organizing']\n",
    "list_three = ['universe', 'university']\n",
    "\n",
    "# calling stem_lemm\n",
    "stem_lemm(list_one)\n",
    "stem_lemm(list_two)\n",
    "stem_lemm(list_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use?\n",
    "When to use Stop Words? Lemmanization? Stemming? \n",
    "\n",
    "Using these methods are a way of throwing away information and truly depeneds on the model you use. If you model allows for more complexity - such as utilizing a deep learning model - then it would be best not to use these. \n",
    "\n",
    "As they can hurt your performance if using deep learning.\n",
    "\n",
    "Therefore the convention is to use them for smaller models - which is often the case when you are dealing with much smaller datasets (unless you use transfer learning). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Next, SciKit Learn has a method that will extract all the word counts for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating\n",
    "vectorizer = CountVectorizer(stop_words='english') # removes stop words by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating vector from our train data\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data).todense()\n",
    "\n",
    "# checking the shape\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above our shape of the vector is (2034, 26576). We can view this as: 2034 different postings (rows), 26576 tokens/words (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26576,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to support this: let's check the size of our vocab from the vector\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cosmonauts', 'cosmos', 'cosponsored', 'cost', 'costa', 'costar',\n",
       "       'costing', 'costly', 'costruction', 'costs', 'cosy', 'cote',\n",
       "       'couched', 'couldn', 'council', 'councils', 'counsel',\n",
       "       'counselees', 'counselor', 'count'], dtype='<U80')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[7000:7020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on SVD\n",
    "We would clearly expect that words that appear most frequently in one topic appear less frequently in the other - otherwise that word wouldn't make a good choice to seperate out the two topics. Therefore, we expect the topics to be **orthogonal**. \n",
    "\n",
    "The SVD algorithm factorizes a matrix into one matrix with **orthogonal columns** and one with **orthogonal rows** (along with a diagonal matrix, which contains the **relative importance** of each factor). \n",
    "\n",
    "#### Orthogonal Matrix\n",
    "An orthogonal matrix is a *square matrix* whose columns and rows are *orthogonal unit vectors* \n",
    "$$Q^TQ = QQ^T = I$$\n",
    "\n",
    "Where $I$ iss the identity matrix. \n",
    "\n",
    "This leads to the equibalent characterization: a matrix Q is orthogonal if its *transpose* is equal to its *inverse*\n",
    "\n",
    "$$Q^T = Q^{-1}$$\n",
    "\n",
    "SVD is an **exact decomposition**, since the matrices it creates are big enough to fully cover the original matrix. SVD is extremely widely used in linear algebra, and specifically in data science including: \n",
    "* Semantic analysis\n",
    "* Collaborative Filterting/Recommendation\n",
    "* Calculate Moore-Penrose pseudoinverse\n",
    "* Data compression\n",
    "* Principal componenet analysis\n",
    "\n",
    "Latent Semantic Analysis (LSA) uses SVD. You will sometimes hear topic modelling reffered to as LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 3.24 s, total: 1min 23s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "# vectors: Input Matrix\n",
    "# U: matrix\n",
    "# s: singular values \n",
    "# Vh: matrix\n",
    "%time U, s, Vh = linalg.svd(vectors, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 2034) (2034,) (2034, 26576)\n"
     ]
    }
   ],
   "source": [
    "print(U.shape, s.shape, Vh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([433.92698542, 291.51012741, 240.71137677, 220.00048043])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[433.92698542,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        , 291.51012741,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        , 240.71137677,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , 220.00048043]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s is shown as a vector but we can use np.diag() to create our diagonal matrix\n",
    "np.diag(s[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x109066dd8>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting our singular values\n",
    "plt.plot(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show topics from SVD - Concepts we learned\n",
    "num_top_words  = 8 # How many words to show\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t)for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['critus ditto propagandist surname galacticentric kindergarten surreal imaginative',\n",
       " 'jpeg gif file color quality image jfif format',\n",
       " 'graphics edu pub mail 128 3d ray ftp',\n",
       " 'jesus god matthew people atheists atheism does graphics',\n",
       " 'image data processing analysis software available tools display',\n",
       " 'god atheists atheism religious believe religion argument true',\n",
       " 'space nasa lunar mars probe moon missions probes',\n",
       " 'image probe surface lunar mars probes moon orbit',\n",
       " 'argument fallacy conclusion example true ad argumentum premises',\n",
       " 'space larson image theory universe physical nasa material']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(Vh[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is still confusion from seeing the above. What we have essentially done is the following: We gathered 2000 documents from (4) pre-determined topics. From each document we gathered all of the vocab and then created a large matrix (vectors) of size (m, n) where m represents the document and n represents the word with each element within the matrix representing how many times (frequency) that single word appeared in the document. \n",
    "\n",
    "Then with doing SVD - we model our 'concepts' and it does this simply by knowing the different documents and all the words within the docouments (freq). \n",
    "\n",
    "So show_topics returns a list of vocabularly (num of words) with each element representing a different cluster or 'concept' \n",
    "\n",
    "Therefor if you read the second element: 'jpeg gif file color quality image jfif format' - we know from our knowledge of the original database that this unsupervised learning task has learned these words fall into a similar topic. \n",
    "\n",
    "The model doesn't know this topic is 'graphics' but we do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)\n",
    "Rather than constraining our factors to be *orthogonal*, another idea would be to constrain them to be non-negative. NMF is a factorization of a non-negative data set: $V$: \n",
    "\n",
    "$$V = W H$$\n",
    "\n",
    "Into non-negative matrices $W, H$. Often positive factors will be **more easily iterpretable**. (how do you explain a negative value? - problem with SVD).\n",
    "\n",
    "So if $V$ was vectorized images. Then this can be represented as $W$ being facial features times $H$ as the importance of features in each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = vectors.shape\n",
    "d = 5 # number of topics / concepts we want to learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decomposition.NMF(n_components=d, random_state=1)\n",
    "\n",
    "W1 = clf.fit_transform(vectors) # Matrix one \n",
    "H1 = clf.components_            # Matrix two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jpeg image gif file color images format quality',\n",
       " 'edu graphics pub mail 128 ray ftp send',\n",
       " 'space launch satellite nasa commercial satellites year market',\n",
       " 'jesus god people matthew atheists does atheism said',\n",
       " 'image data available software processing ftp edu analysis']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing our topics learned - 5\n",
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're having confusion as to what is happening above. Remember ```vectors``` is simply a matrix of size (m,n) where ```m``` represents document and ```n``` represents the word in that document. \n",
    "\n",
    "Therefor each element is the frequency of that word in that specific document. \n",
    "\n",
    "```d``` is the number of topics/concepts we want to learn. \n",
    "\n",
    "```W1``` is the first decomposed matrix of size (m, r) where ```m``` is the document and ```r``` is the concept. In our case ```r``` = ```d``` so this matrix is learning 5 concepts or \"topics\". \n",
    "\n",
    "```H1``` is the second decomposed matrix of size (r, n) where ```r``` is the \"topic\" or concept to learn, while ```n``` is the word. \n",
    "\n",
    "So below lets check the dimensions to further weigh our intuition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: (2034, 5)\n",
      "H1: (5, 26576)\n"
     ]
    }
   ],
   "source": [
    "print(f'W1: {W1.shape}')\n",
    "print(f'H1: {H1.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with ```show_topics(a)``` we are essentially displaying words that fall into the \"topic\" or concept learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD\n",
    "We saved a lot of time when we calculated NMF by only calculating the subset of columns we were interested in. Is there a way to get this benefit with SVD? \n",
    "\n",
    "This is what Truncated SVD does: We are just interested in the vectors corresponding to the **largest** singular values. \n",
    "\n",
    "### Shortcomings of classical algorithms for decompisition\n",
    "* Matrices are \"Stupendously big\" \n",
    "* Data are often **missing or inaccurate**. Why spend extra computational resources when imprecision of input limits precision of the output?\n",
    "* **data transfer** now plays a major role in time of algorithms. Techniques that require fewer passes over the data may be substantially faster, even if they require more flops (Flops = floating point operations)\n",
    "\n",
    "### Advantages of randomized algorithms\n",
    "* Inherently stable\n",
    "* Performance guarantees do not depend on subtle spectral properties\n",
    "* Needed matrix-vector products can be done in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
